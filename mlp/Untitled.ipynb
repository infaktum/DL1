{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e8fec6-63de-4dae-9303-fa149192abaf",
   "metadata": {},
   "source": [
    "### **Mathematische Erkl√§rung der Wissensspeicherung in einem MLP**  \n",
    "\n",
    "Ein **Multilayer Perceptron (MLP)** besteht aus mehreren Schichten von Neuronen. Die Verbindungen zwischen den Neuronen sind mit **Gewichten** \\( W \\) versehen, die w√§hrend des Trainings angepasst werden, um Muster zu lernen.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Grundstruktur eines MLP**  \n",
    "Ein MLP mit einer Eingabeschicht, einer versteckten Schicht und einer Ausgabeschicht folgt dieser Struktur:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\rightarrow \\text{Eingabeschicht} \\rightarrow \\text{Versteckte Schicht} \\rightarrow \\text{Ausgabeschicht} \\rightarrow \\hat{\\mathbf{y}}\n",
    "$$\n",
    "\n",
    "Jede Schicht f√ºhrt eine **lineare Transformation** mit einer Aktivierungsfunktion durch:\n",
    "\n",
    "### **Vorw√§rtspropagation**  \n",
    "F√ºr eine Schicht \\( l \\) mit Eingabe \\( \\mathbf{x} \\), Gewichten \\( W^{(l)} \\) und Bias \\( b^{(l)} \\) wird der **Netzinput** \\( \\mathbf{z}^{(l)} \\) berechnet als:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(l)} = W^{(l)} \\mathbf{x} + b^{(l)}\n",
    "$$\n",
    "\n",
    "Anschlie√üend wird eine **Aktivierungsfunktion** \\( \\sigma \\) (z. B. ReLU oder Sigmoid) angewendet:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "Die Aktivierung \\( \\mathbf{a}^{(l)} \\) wird zur Eingabe der n√§chsten Schicht.\n",
    "\n",
    "### **Beispiel f√ºr eine vollst√§ndige MLP-Berechnung**\n",
    "Angenommen, wir haben:\n",
    "- Eingabe \\( \\mathbf{x} = [x_1, x_2]^\\top \\)\n",
    "- Eine versteckte Schicht mit zwei Neuronen und Gewichten:\n",
    "\n",
    "  \\[\n",
    "  W^{(1)} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix}, \\quad b^{(1)} = \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}\n",
    "  \\]\n",
    "\n",
    "- Aktivierung in der versteckten Schicht:\n",
    "\n",
    "  \\[\n",
    "  \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + b^{(1)}\n",
    "  \\]\n",
    "\n",
    "  \\[\n",
    "  \\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)})\n",
    "  \\]\n",
    "\n",
    "- Eine Ausgabeschicht mit Gewichten \\( W^{(2)} \\):\n",
    "\n",
    "  \\[\n",
    "  W^{(2)} = \\begin{bmatrix} w_{31} & w_{32} \\end{bmatrix}, \\quad b^{(2)} = [b_3]\n",
    "  \\]\n",
    "\n",
    "- Ausgabe:\n",
    "\n",
    "  \\[\n",
    "  z^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + b^{(2)}\n",
    "  \\]\n",
    "\n",
    "  \\[\n",
    "  \\hat{y} = \\sigma(z^{(2)})\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Wie speichert das MLP Wissen?**  \n",
    "\n",
    "Das Wissen des Netzwerks ist in den Gewichten \\( W^{(l)} \\) und Biases \\( b^{(l)} \\) gespeichert. Diese Parameter werden durch **Backpropagation und Gradientenabstieg** optimiert.\n",
    "\n",
    "### **Backpropagation und Gewichtsanpassung**\n",
    "1. **Fehlermessung:** Der Fehler wird durch eine Verlustfunktion \\( L \\) (z. B. Mean Squared Error oder Cross-Entropy) berechnet:\n",
    "\n",
    "   \\[\n",
    "   L = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Loss}(\\hat{y}_i, y_i)\n",
    "   \\]\n",
    "\n",
    "2. **Gradientenberechnung:**  \n",
    "   Mit der **Kettenregel** wird der Gradient des Fehlers nach den Gewichten bestimmt:\n",
    "\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\n",
    "   \\]\n",
    "\n",
    "3. **Gradientenabstieg:**  \n",
    "   Die Gewichte werden mit der Lernrate \\( \\eta \\) aktualisiert:\n",
    "\n",
    "   \\[\n",
    "   W^{(l)} := W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "   \\]\n",
    "\n",
    "Durch diesen Prozess passen sich die Gewichte schrittweise an, sodass das Netzwerk Muster in den Daten erkennt.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Beispiel f√ºr Wissen in Gewichten**  \n",
    "Angenommen, ein trainiertes MLP erkennt Katzen und Hunde anhand von Pixelwerten eines Bildes.  \n",
    "- Ein Neuron in der ersten Schicht k√∂nnte lernen, **Ohrenformen** zu erkennen.  \n",
    "- Ein anderes Neuron k√∂nnte lernen, **Augenabst√§nde** zu unterscheiden.  \n",
    "- Die letzte Schicht k√∂nnte eine hohe Aktivierung f√ºr ‚ÄûKatze‚Äú oder ‚ÄûHund‚Äú ausgeben.  \n",
    "\n",
    "Die trainierten **Gewichtswerte speichern dieses Wissen**, indem sie die entscheidenden Merkmale betonen.\n",
    "\n",
    "---\n",
    "\n",
    "### **Fazit**  \n",
    "Das Wissen eines MLP ist in den **Gewichten und Biases** gespeichert, die durch Training optimiert werden. Diese Parameter steuern, wie Eingaben verarbeitet werden, um die gew√ºnschte Ausgabe zu erzeugen. **Mathematisch ausgedr√ºckt, sind die Gewichte die Koeffizienten einer komplexen, nicht-linearen Funktion, die Muster aus Daten abstrahiert.**  \n",
    "\n",
    "M√∂chtest du noch tiefer in das Thema eintauchen, z. B. mit konkretem Code zur Visualisierung der Gewichte? üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beb800-0e66-4ce0-ab5d-235a23135937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
